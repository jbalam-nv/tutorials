{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-finetuning/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to fine-tune a Riva ASR Acoustic Model (Citrinet) with TAO Toolkit\n",
    "This tutorial walks you through how to fine-tune a Riva ASR acoustic model (Citrinet) with TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we are going to discuss the Citrinet model, which is an end-to-end ASR model that takes in audio and produces text.\n",
    "\n",
    "Citrinet is a descendent of QuartzNet that features the squeeze-and-excitation (SE) block and sub-word tokenization and has a better accuracy/performance than QuartzNet.\n",
    "\n",
    "![CitriNet with CTC](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/_images/citrinet_vertical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ASR using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case so these directories are correctly visible to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory for this tutorial\n",
    "WORKING_DIR = 'asr_am_finetuning'\n",
    "\n",
    "# Defining paths on the local host machine\n",
    "%env HOST_DATA_DIR = {WORKING_DIR}/data\n",
    "%env HOST_SPECS_DIR = {WORKING_DIR}/specs\n",
    "%env HOST_RESULTS_DIR = {WORKING_DIR}/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories on the local host machine\n",
    "! mkdir -p $WORKING_DIR\n",
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"128G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set the encryption key and use the same key for all commands.\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command.<br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the specs directory if it is already there to avoid errors\n",
    "! tao speech_to_text_citrinet download_specs \\\n",
    "    -r $RESULTS_DIR/speech_to_text_citrinet \\\n",
    "    -o $SPECS_DIR/speech_to_text_citrinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the Nigerian English speech dataset to evaluate and fine-tune our acoustic model. The Nigerian English speech dataset is available [here](https://www.openslr.org/70/). This data set contains transcribed high-quality audio of Nigerian English sentences recorded by volunteers, in Lagos Nigerian and in London. \n",
    "\n",
    "Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the dataset exists, otherwise download it\n",
    "if os.path.exists(os.environ[\"HOST_DATA_DIR\"] + '/en_ng_female.zip'):\n",
    "    print(\"Dataset exists, skipping download\")\n",
    "else:\n",
    "    print(\"Dataset does not exist, downloading\")\n",
    "    !wget 'https://www.openslr.org/resources/70/en_ng_female.zip' -P $HOST_DATA_DIR\n",
    "    !wget 'https://www.openslr.org/resources/70/en_ng_male.zip'   -P $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untar the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the finetuning data\n",
    "# Ensure that the unzip utility is available. If not, install it.\n",
    "!unzip -nq $HOST_DATA_DIR/en_ng_female.zip -d $HOST_DATA_DIR/en_ng_female\n",
    "!mv $HOST_DATA_DIR/en_ng_female/line_index.tsv $HOST_DATA_DIR/en_ng_female/line_index_female.tsv\n",
    "!unzip -nq $HOST_DATA_DIR/en_ng_male.zip -d $HOST_DATA_DIR/en_ng_male\n",
    "!mv $HOST_DATA_DIR/en_ng_male/line_index.tsv $HOST_DATA_DIR/en_ng_male/line_index_male.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nigerian-English speech dataset contains transcripts in the format `(fileID) <t> <s> transcript </s>`, where:\n",
    "1. `(fileID)` - denotes the name of the .wav file corresponding to this transcript\n",
    "2. `<t>` - denotes tab space\n",
    "3. `<s>` - denotes the start of the transcript\n",
    "4. `</s>` - denotes the end of the transcript\n",
    "\n",
    "The audio files are in `.wav` format. The dataset also needs to be split in train and test set. We'll select a ratio of 90:10 for the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to extract the relevant information from the `.tsv` metadata files included with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "\n",
    "def process_en_ng_tsvs(host_data_dir, data_dir):\n",
    "    genders = ['female','male']\n",
    "    entries = []\n",
    "    # Extract the relevant information from the tsv files\n",
    "    for gender in genders: \n",
    "        dataset  = f'en_ng_{gender}'\n",
    "        tsv_name = f'line_index_{gender}.tsv'\n",
    "        tsv_file = os.path.join(host_data_dir, dataset, tsv_name)\n",
    "        with open(tsv_file, encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                label, text = line[: line.index(\"\\t\")], line[line.index(\"\\t\") + 1 :]\n",
    "                speaker_id  = label.split('_')[1]\n",
    "                host_wav_file = os.path.join(host_data_dir, dataset, label + '.wav')\n",
    "                wav_file = os.path.join(data_dir, dataset, label + '.wav')\n",
    "                transcript_text = text.lower().strip()\n",
    "\n",
    "                # check duration\n",
    "                wf = wave.open(host_wav_file,'r')\n",
    "                frames, rate = wf.getnframes(), wf.getframerate()\n",
    "                duration = round(frames / float(rate), 4)\n",
    "                \n",
    "                entry = {}\n",
    "                entry['audio_filepath'] = wav_file\n",
    "                entry['duration'] = float(duration)\n",
    "                entry['text'] = transcript_text\n",
    "                entry['gender'] = gender\n",
    "                entry['speaker_id'] = speaker_id\n",
    "                entries.append(entry)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TAO/NeMo format, the dataset consists of a set of utterances in individual audio files (.wav) and a manifest that describes the dataset, with information about one utterance per line.<br>\n",
    "Each line of the manifest should be in the following format:\n",
    "\n",
    "```\n",
    "{\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription of the utterance\", \"duration\": 23.147}\n",
    "```\n",
    "\n",
    "The `audio_filepath` field should provide an absolute path to the .wav file corresponding to the utterance. The `text` field should contain the full transcript for the utterance, and the `duration` field should reflect the duration of the utterance in seconds.\n",
    "\n",
    "Other metadata fields like `gender` and `speaker_id` can be added in the manifest but are not useful for finetuning the acoustic model.\n",
    "\n",
    "We will define a function to generate `manifest.json` file from the `.tsv` metadata files included with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def generate_en_ng_manifest(host_data_dir, data_dir, random_seed=0, val_split=0.1):\n",
    "    # Extract the relevant information from the tsv files\n",
    "    entries = process_en_ng_tsvs(host_data_dir, data_dir)\n",
    "    # Generate the manifest files\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(entries)\n",
    "    num_val_entries = int(val_split * len(entries))\n",
    "    ft_manifest_file  = os.path.join(host_data_dir, 'en_ng_ft_manifest.json')\n",
    "    val_manifest_file = os.path.join(host_data_dir, 'en_ng_val_manifest.json')\n",
    "    with open(ft_manifest_file, 'w') as fout:\n",
    "        for m in entries[:-num_val_entries]:\n",
    "            fout.write(json.dumps(m) + '\\n')\n",
    "    with open(val_manifest_file, 'w') as fout:\n",
    "        for m in entries[-num_val_entries:]:\n",
    "            fout.write(json.dumps(m) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate the manifest files for the Nigerian English Speech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_en_ng_manifest(os.environ[\"HOST_DATA_DIR\"], DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to a sample audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path of the file here to listen to some other audio\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/en_ng_female/ngf_05223_00457923143.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do the actual finetuning, we will tokenize the text.\n",
    "TAO provides implementation of 2 SubWord tokenization techniques - WordPiece (WP) and SentencePiece (SP).<br>\n",
    "For SentencePiece, TAO also provides the option to select between these different types - unigram, bpe, char & word.\n",
    "\n",
    "Subword tokenization creates a subword vocabulary for the text. The core concept behind subwords is that frequently occurring words should be in the vocabulary, whereas rare words should be split into frequent sub words. Eg. The word “refactoring” can be split into “re”, “factor”, and “ing”. \n",
    "\n",
    "For training Citrinet, we use the `create_tokenizer` command to create the tokenizer that generates the unigram SP subword vocabulary. <br>\n",
    "The `create_tokenizer.yaml` contains the following specifications for tokenization:\n",
    "```\n",
    "vocab_size: 1024\n",
    "tokenizer:\n",
    "    tokenizer_type: \"spe\"\n",
    "    spe_type: \"unigram\"\n",
    "    spe_character_coverage: 1.0\n",
    "    lower_case: False\n",
    "```\n",
    "\n",
    "BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization.<br>\n",
    "WordPiece is similar to BPE since it includes all the characters and symbols into its base vocabulary first. BPE and WordPiece lies in the way the symbol pairs are chosen for adding to the vocabulary.\n",
    "\n",
    "Unigram tokenization also starts with setting a desired vocabulary size. However, the main difference between unigram and the previous 2 approaches is that we don’t start with a base vocabulary of characters only. Instead, the base vocabulary has all the words and symbols. \n",
    "\n",
    "All the tokenizers above assume that space separates words. This is true except for a few languages like Chinese, Japanese etc. SentencePiece does not treat space as a separator, instead, it takes the string as input in its original raw format, i.e. along with all spaces. It then uses BPE or unigram as its tokenizers to construct the vocabulary.\n",
    "\n",
    "Feel free to read [HuggingFace's blog](https://huggingface.co/docs/transformers/tokenizer_summary) to learn more about tokenization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet create_tokenizer \\\n",
    "-e $SPECS_DIR/speech_to_text_citrinet/create_tokenizer.yaml \\\n",
    "-r $RESULTS_DIR/citrinet/create_tokenizer \\\n",
    "manifests=$DATA_DIR/en_ng_ft_manifest.json \\\n",
    "output_root=$RESULTS_DIR/ \\\n",
    "vocab_size=55 # to create an apt vocab for acoustic model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data and the tokenizer ready, let's download the pre-trained Citrinet checkpoint that we will use for finetuning. We will download the ASR model, [Citrinet-1024](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_citrinet), that is used in Riva ASR Speech skill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the checkpoint exists, otherwise download it\n",
    "if os.path.exists(os.environ[\"HOST_RESULTS_DIR\"] + '/speechtotext_en_us_citrinet_vtrainable_v3.0/'):\n",
    "    print(\"Checkpoint exists, skipping download\")\n",
    "else:\n",
    "    print(\"Checkpoint does not exist, downloading\")\n",
    "    ! ngc registry model download-version \"nvidia/tao/speechtotext_en_us_citrinet:trainable_v3.0\"\n",
    "    ! mv speechtotext_en_us_citrinet_vtrainable_v3.0/ $HOST_RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The fine-tune spec file (`$SPECS_DIR/finetune.yaml`) contain specifics to fine-tune the English AM model, that we just downloaded, to Russian language (also called as language adaptation). In order to fine-tune the model for English language (Nigerian-English speech dataset is an English ASR dataset), we will modify that spec file.\n",
    "\n",
    "Here is the minimal spec file that we will use for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile finetune_en.yaml\n",
    "\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "# TLT spec file for fine-tuning a previously trained ASR models based on CTC over the MCV Russian dataset.\n",
    "\n",
    "trainer:\n",
    "  max_epochs: 3   # This is low for demo purposes\n",
    "\n",
    "tlt_checkpoint_interval: 1\n",
    "\n",
    "# Whether or not to change the decoder vocabulary.\n",
    "# Note that this MUST be set if the labels change, e.g. to a different language's character set\n",
    "# or if additional punctuation characters are added.\n",
    "change_vocabulary: false   # CHANGED TO FALSE\n",
    "\n",
    "tokenizer:\n",
    "  dir: ???\n",
    "  type: \"bpe\"  # Can be either bpe or wpe\n",
    "\n",
    "# Fine-tuning settings: training dataset\n",
    "finetuning_ds:\n",
    "  manifest_filepath: ???\n",
    "  sample_rate: 16000\n",
    "  batch_size: 16\n",
    "  trim_silence: true\n",
    "  max_duration: 16.7\n",
    "  shuffle: true\n",
    "  is_tarred: false\n",
    "  tarred_audio_filepaths: null\n",
    "\n",
    "# Fine-tuning settings: validation dataset\n",
    "validation_ds:\n",
    "  manifest_filepath: ???\n",
    "  sample_rate: 16000\n",
    "  batch_size: 16\n",
    "  shuffle: false\n",
    "\n",
    "# Fine-tuning settings: optimizer\n",
    "optim:\n",
    "  name: novograd\n",
    "  lr: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the above created specs file\n",
    "!mv finetune_en.yaml $HOST_SPECS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acoustic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finetuning an ASR Citrinet model in TAO, we use the `tao speech_to_text_citrinet finetune` command with the following arguments:\n",
    "<ul>\n",
    "    <li>`-e`: Path to the spec file </li>\n",
    "    <li>`-g`: Number of GPUs to use </li>\n",
    "    <li>`-r`: Path to the results folder </li>\n",
    "    <li>`-m`: Path to the model </li>\n",
    "    <li>`-k`: User specified encryption key to use while saving/loading the model </li>\n",
    "    <li>Any overrides to the spec file. For example, `trainer.max_epochs`. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet finetune \\\n",
    "     -e $SPECS_DIR/finetune_en.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/speechtotext_en_us_citrinet_vtrainable_v3.0/speechtotext_en_us_citrinet.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/finetune \\\n",
    "     finetuning_ds.manifest_filepath=$DATA_DIR/en_ng_ft_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/en_ng_val_manifest.json \\\n",
    "     trainer.max_epochs=5 \\\n",
    "     finetuning_ds.num_workers=20 \\\n",
    "     validation_ds.num_workers=20 \\\n",
    "     tokenizer.dir=$RESULTS_DIR/tokenizer_spe_unigram_v55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ASR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model trained, we need to check how well it performs. Let's first evaluate the pre-trained model on this validation set to check the WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet evaluate \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/evaluate.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/speechtotext_en_us_citrinet_vtrainable_v3.0/speechtotext_en_us_citrinet.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/evaluate-pretrained \\\n",
    "     test_ds.manifest_filepath=$DATA_DIR/en_ng_val_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained model scores **20.01 WER** on the validation set. \n",
    "\n",
    "Word Error Rate is a measure of how accurate an ASR system performs. Quite literally, it calculates how many “errors” are in the transcription text produced by an ASR system, compared to a human transcription. The lower the number, the better.\n",
    "\n",
    "Now, let's evaluate the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet evaluate \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/evaluate.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/evaluate \\\n",
    "     test_ds.manifest_filepath=$DATA_DIR/en_ng_val_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will observe that the model scores a **18.89** word error rate (WER) on the Nigerian-English validation set.<br>\n",
    "We've been able to gain an approx **5%** boost in the WER from 20.01 -> 18.89 by just fine-tuning for 5 epochs.\n",
    "\n",
    "Feel free to try finetuning for more than 5 epochs to see much better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ASR model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using NVIDIA Riva; a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file.\n",
    "\n",
    "This exported .riva model will be used in the next notebook for deploying the ASR pipeline with this customized accoustic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet export \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/riva \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=asr-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX (Note: Export to ONNX is not needed for Riva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet export \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/export \\\n",
    "     export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ASR Inference using the checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASR Inference with TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to run inference on the tlt checkpoint with TAO Toolkit. \n",
    " For real-time inference and best latency, we need to deploy this model on Riva, which would be covered in the next tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets listen to the audio first\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/en_ng_male/ngm_09697_00751039644.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the ground truth transcript for this sample\n",
    "import json\n",
    "\n",
    "def read_manifest(path):\n",
    "    manifest = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            data = json.loads(line)\n",
    "            manifest.append(data)\n",
    "    return manifest\n",
    "\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/en_ng_val_manifest.json'\n",
    "path_in_manifest = DATA_DIR + '/en_ng_male/ngm_09697_00751039644.wav'\n",
    "\n",
    "manifest = read_manifest(path)\n",
    "transcript = [x['text'] for x in manifest if x[\"audio_filepath\"] == path_in_manifest]\n",
    "\n",
    "print(\"Ground truth transcript: \", transcript)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions using the pre-trained model\n",
    "!tao speech_to_text_citrinet infer \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/speechtotext_en_us_citrinet_vtrainable_v3.0/speechtotext_en_us_citrinet.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/infer-pretrained \\\n",
    "     file_paths=[$DATA_DIR/en_ng_male/ngm_09697_00751039644.wav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions using the finetuned model\n",
    "!tao speech_to_text_citrinet infer \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/infer \\\n",
    "     file_paths=[$DATA_DIR/en_ng_male/ngm_09697_00751039644.wav]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would observe, the predicted transcript from the finetuned model is more closer to the grouth truth transcript. The words \"shows\" and \"all\" are wrongly predicted as \"shoes\" and \"old\" by the pre-trained model.\n",
    "\n",
    "You can upload your recorded `.wav` file and provide its path to the `file_paths` argument in the cell above to get the transcribed speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fine-tuned Citrinet accoustic model, we can now deploy this custom model to NVIDIA Riva.\n",
    "\n",
    "Make sure to keep the path of `asr-model.riva` handy for deployment i.e. `asr_am_finetuning/results/citrinet/riva/asr-model.riva`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
