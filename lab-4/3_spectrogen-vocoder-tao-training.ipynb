{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_tts_tts-python-advanced-pretrain-tts-tao-training/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to train Riva TTS models (FastPitch and HiFiGAN) with TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through the steps to train Riva TTS models (FastPitch and HiFiGAN) from scratch with LJSpeech dataset using TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we will customize the Riva TTS pipeline by training Riva TTS models with NVIDIA's TAO Toolkit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective is to synthesize reasonable and natural speech for given text. Since there are no universal standards to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained.\n",
    "\n",
    "TTS consists of two models: [FastPitch](https://arxiv.org/pdf/2006.06873.pdf) and [HiFi-GAN](https://arxiv.org/pdf/2010.05646.pdf).\n",
    "\n",
    "* FastPitch is spectrogram model that generates a Mel spectrogram from text input. It's a fully-parallel text-to-speech model based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference, and generates speech that could be further controlled with predicted contours. FastPitch can thus change the perceived emotional state of the speaker or put emphasis on certain lexical units\n",
    "\n",
    "![FastPitch](./imgs/architecture-fastpitch.PNG)\n",
    "\n",
    "\n",
    "* HiFiGAN is a vocoder model that generates an audio output from the Mel spectrograms generated using FastPitch. HiFiGAN uses an end-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain. It relies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech. The proposed model generalizes well to new speakers, new speech content, and new environments. It significantly outperforms state-of-the-art baseline methods in both objective and subjective experiments. \n",
    "\n",
    "![HiFiGAN](./imgs/architecture-hifigan.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case so these directories are correctly visible to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory for this tutorial\n",
    "WORKING_DIR = 'tts_training'\n",
    "\n",
    "# Defining paths on the local host machine\n",
    "%env HOST_DATA_DIR = {WORKING_DIR}/data\n",
    "%env HOST_SPECS_DIR = {WORKING_DIR}/specs\n",
    "%env HOST_RESULTS_DIR = {WORKING_DIR}/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $WORKING_DIR\n",
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"128G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key and use the same key for all commands:\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spec files for FastPitch\n",
    "! tao spectro_gen download_specs \\\n",
    "    -r $RESULTS_DIR/spectro_gen \\\n",
    "    -o $SPECS_DIR/spectro_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spec files for HiFiGAN\n",
    "! tao vocoder download_specs \\\n",
    "    -r $RESULTS_DIR/vocoder \\\n",
    "    -o $SPECS_DIR/vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the popular [LJSpeech](https://keithito.com/LJ-Speech-Dataset/) dataset. Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the dataset exists, otherwise download it\n",
    "if os.path.exists(os.environ[\"HOST_DATA_DIR\"] + '/ljspeech.tar.bz2'):\n",
    "    print(\"Dataset exists, skipping download\")\n",
    "else:\n",
    "    print(\"Dataset does not exist, downloading\")\n",
    "    ! wget -O $HOST_DATA_DIR/ljspeech.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, untar the dataset and move it to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tar -xf $HOST_DATA_DIR/ljspeech.tar.bz2\n",
    "! rm -rf $HOST_DATA_DIR/ljspeech\n",
    "! mv LJSpeech-1.1 $HOST_DATA_DIR/ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TAO/NeMo format, the dataset consists of a set of utterances in individual audio files (.wav) and a manifest that describes the dataset, with information about one utterance per line.<br>\n",
    "Each line of the manifest should be in the following format:\n",
    "\n",
    "```\n",
    "{\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription of the utterance\", \"duration\": 23.147}\n",
    "```\n",
    "\n",
    "The `audio_filepath` field should provide an absolute path to the .wav file corresponding to the utterance. The `text` field should contain the full transcript for the utterance, and the `duration` field should reflect the duration of the utterance in seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pre-processing step downloads audio (.wav) to transcript (.txt) file lists from NVIDIA for LJSpeech dataset and generate the manifest files in the format mentioned above. \n",
    "\n",
    "Let's run the below command to pre-process LJSpeech.\n",
    "\n",
    "Be patient! This step can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tao spectro_gen dataset_convert \\\n",
    "    -e $SPECS_DIR/spectro_gen/dataset_convert_ljs.yaml \\\n",
    "    -r $RESULTS_DIR/spectro_gen/dataset_convert \\\n",
    "    data_dir=$DATA_DIR/ljspeech \\\n",
    "    dataset_name=ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO interface enables you to configure the training parameters from the command-line interface. <br>\n",
    "\n",
    "The process of opening the training script, finding the parameters of interest (which might be spread across multiple files), and making the changes needed, is being replaced by a simple command-line interface.\n",
    "\n",
    "For example, if the number of epochs are needed to be modified along with a change in the learning rate, you can add `trainer.max_epochs=10` and `optim.lr=0.02` and train the model. Sample commands are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training TTS models in TAO, we use the `tao spectro_gen train` and `tao vocoder train` commands with the following arguments:\n",
    "<ul>\n",
    "    <li>`-e`: Path to the spec file </li>\n",
    "    <li>`-g`: Number of GPUs to use </li>\n",
    "    <li>`-r`: Path to the results folder </li>\n",
    "    <li>`-k`: User specified encryption key to use while saving/loading the model </li>\n",
    "    <li>Any overrides to the spec file. For example, `trainer.max_epochs`. </li>\n",
    "</ul>\n",
    "\n",
    "NOTE: In order to get a TTS pipeline, you need to train **BOTH** FastPitch (`spectro_gen`) and HiFi-GAN (`vocoder`). For HiFi-GAN, since it's universal for a specific language, the pretrained weights from NGC will itself give you good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior is needed for FastPitch training. If an empty folder is provided, prior will generate on-the-fly.\n",
    "# Prior alignment matrix is used to align speech and text inputs.\n",
    "! mkdir -p $HOST_RESULTS_DIR/spectro_gen/train/prior_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen train \\\n",
    "     -e $SPECS_DIR/spectro_gen/train.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/spectro_gen/train \\\n",
    "     train_dataset=$DATA_DIR/ljspeech/ljspeech_train.json \\\n",
    "     validation_dataset=$DATA_DIR/ljspeech/ljspeech_val.json \\\n",
    "     prior_folder=$RESULTS_DIR/spectro_gen/train/prior_folder \\\n",
    "     trainer.max_epochs=5 \\\n",
    "     train_ds.num_workers=12 \\\n",
    "     validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing `trainer.max_epochs`, HiFi-GAN requires the definition of `trainer.max_steps`. Defining `trainer.max_epochs` for HiFi-GAN has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder train \\\n",
    "     -e $SPECS_DIR/vocoder/train.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/vocoder/train \\\n",
    "     train_dataset=$DATA_DIR/ljspeech/ljspeech_train.json \\\n",
    "     validation_dataset=$DATA_DIR/ljspeech/ljspeech_val.json \\\n",
    "     trainer.max_steps=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS Inference with TAO Toolkit\n",
    "\n",
    "In this section, we are going to run inference on the trained TTS models. As previously mentioned, since there are no universal standards to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained. Therefore, we do not provide `evaluate` functionality in TAO Toolkit for TTS but only provide `infer` functionality.\n",
    "\n",
    "The inference in the following cells is not optimized for real-time performance. For real-time inference and best latency, we would deploy this model using RIVA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Inference with TLT checkpoint\n",
    "\n",
    "In this section, we will run inference on the `.tlt` checkpoint trained with TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate spectrogram\n",
    "\n",
    "The first step for inference is generating a spectrogram. That's a NumPy array (saved as `.npy` file) for a sentence which can be converted to voice by a vocoder. We use the FastPitch model we just trained to generate a spectrogram.\n",
    "\n",
    "You may have to work with the `infer.yaml` file to set the texts you want for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen infer \\\n",
    "     -e $SPECS_DIR/spectro_gen/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/infer \\\n",
    "     output_path=$RESULTS_DIR/spectro_gen/infer/spectro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sound file\n",
    "\n",
    "The second step for inference is generating a `.wav` sound file based on a spectrogram you generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder infer \\\n",
    "     -e $SPECS_DIR/vocoder/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/infer \\\n",
    "     input_path=$RESULTS_DIR/spectro_gen/infer/spectro \\\n",
    "     output_path=$RESULTS_DIR/vocoder/infer/wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "# change path of the file here\n",
    "ipd.Audio(os.environ[\"HOST_RESULTS_DIR\"] + '/vocoder/infer/wav/0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TTS model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using NVIDIA Riva; a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to RIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen export \\\n",
    "     -e $SPECS_DIR/spectro_gen/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/export \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=spectro_gen.riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao vocoder export \\\n",
    "     -e $SPECS_DIR/vocoder/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/export \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=vocoder.riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving FastPitch and HiFiGAN .riva models to a common folder\n",
    "# This is required for Riva to build a TTS pipeline\n",
    "! mkdir -p $HOST_RESULTS_DIR/riva\n",
    "! cp $HOST_RESULTS_DIR/spectro_gen/export/spectro_gen.riva $HOST_RESULTS_DIR/riva/\n",
    "! cp $HOST_RESULTS_DIR/vocoder/export/vocoder.riva $HOST_RESULTS_DIR/riva/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained FastPitch and HiFiGAN, we can now deploy these models to NVIDIA Riva.\n",
    "\n",
    "Make sure to keep the path of `spectro_gen.riva` and `vocoder.riva` handy for deployment i.e. `tts_training/results/riva/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d07f1aa6807adb4e3490764d3413abc2e022e4483e8e90f694051be99589cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
