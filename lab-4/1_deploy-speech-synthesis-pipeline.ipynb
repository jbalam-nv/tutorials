{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to deploy a Riva Speech Synthesis Pipeline\n",
    "In this tutorial, you will learn how to deploy Riva speech synthesis models - specifically the **Spectro generation model (FastPitch)** and **Vocoder model (HiFiGAN)** pre-trained models downloaded from NVIDIA NGC. \n",
    "\n",
    "This will serve as a primer for customization tutorials in this lab, which require configuring the Riva speech pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech and natural language understanding services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "- A collection of natural language processing (NLP) services, such as named entity recognition (NER), punctuation, and intent classification.\n",
    "\n",
    "To understand the basics of Riva TTS APIs, refer to [How do I use Riva TTS APIs with out-of-the-box models?](https://github.com/nvidia-riva/tutorials/tree/stable/tts-python-basics.ipynb). <br>\n",
    "\n",
    "For more information about Riva, refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure that you have access to [**NVIDIA NGC**](https://ngc.nvidia.com/signin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Fetch TTS models from NGC\n",
    "### Download the Spectrogram Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FastPitch Spectrogram Generator Model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_english_fastpitch/files). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create a local directory to save models\n",
    "TTS_MODEL_DIR = os.path.join(os.getcwd(), \"tts-models\")\n",
    "!mkdir -p $TTS_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where ngc will download the FastPitch Model\n",
    "SG_DIR = \"speechsynthesis_english_fastpitch_vdeployable_v1.1\"\n",
    "SG_PATH = os.path.join(TTS_MODEL_DIR, SG_DIR)\n",
    "\n",
    "if os.path.exists(SG_PATH):\n",
    "    print(\"Spectrogram generator model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the FastPitch Model\")\n",
    "    !ngc registry model download-version \"nvidia/tao/speechsynthesis_english_fastpitch:deployable_v1.1\" --dest $TTS_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $SG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the Vocoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HiFiGAN Vocoder Model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_hifigan/files). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VC_DIR = \"speechsynthesis_hifigan_vdeployable_v1.0\"\n",
    "VC_PATH = os.path.join(TTS_MODEL_DIR, VC_DIR)\n",
    "\n",
    "if os.path.exists(VC_PATH):\n",
    "    print(\"Vocoder Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the HiFiGAN Model\")\n",
    "    !ngc registry model download-version \"nvidia/tao/speechsynthesis_hifigan:deployable_v1.0\" --dest $TTS_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $VC_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the Auxillary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pronunciation dictionary and abbreviations are also required for speech synthesis pipeline. They are located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechsynthesis_en_us_auxiliary_files/files). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUX_DIR = \"speechsynthesis_en_us_auxiliary_files_vdeployable_v1.3\"\n",
    "AUX_PATH = os.path.join(TTS_MODEL_DIR, AUX_DIR)\n",
    "\n",
    "if os.path.exists(AUX_PATH):\n",
    "    print(\"Auxillary files exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Auxillary files\")\n",
    "    !ngc registry model download-version \"nvidia/tao/speechsynthesis_en_us_auxiliary_files:deployable_v1.3\" --dest $TTS_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $AUX_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the Normalization Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speech synthesis pipeline uses weighted finite-state transducer (WFST) grammars that map strings in written form to strings in spoken form. They are located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/normalization_en_us/files). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NG_DIR = \"normalization_en_us_vdeployable_v1.1\"\n",
    "NG_PATH = os.path.join(TTS_MODEL_DIR, NG_DIR)\n",
    "\n",
    "if os.path.exists(NG_PATH):\n",
    "    print(\"Normalization grammer exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Normalization Grammer\")\n",
    "    !ngc registry model download-version \"nvidia/tao/normalization_en_us:deployable_v1.1\" --dest $TTS_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $NG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components: `riva-build` and `riva-deploy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.4.0-servicemaker\"\n",
    "\n",
    "# Get the ServiceMaker docker\n",
    "! docker pull $RIVA_SM_CONTAINER\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we execute Riva-build to create a pipeline configured for Offline Synthesis. This command for reference is also present in the [pipeline configuration](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-custom.html#riva-build-pipeline-instructions) section of the docs. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set relevant paths relative to where we will mount the models in the Servicemaker docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model paths relative to Riva Servicemaker docker include the _SM suffix\n",
    "\n",
    "TTS_MODEL_DIR_SM = \"/data\" # Path where we mount the downloaded TTS models in the Servicemaker docker\n",
    "\n",
    "# Relative path to Spectrogram Generator Model\n",
    "SG_SM = os.path.join(TTS_MODEL_DIR_SM, SG_DIR, \"FastPitch_Align_22k_LJS_arpa_PitchDuration.riva\")\n",
    "\n",
    "# Relative path to Vocoder model artifacts\n",
    "VC_SM = os.path.join(TTS_MODEL_DIR_SM, VC_DIR, \"HifiGAN_22k_LJS.riva\")\n",
    "\n",
    "# Relative path to Auxillary files\n",
    "ABBR_SM = os.path.join(TTS_MODEL_DIR_SM, AUX_DIR, \"abbr.txt\")\n",
    "PR_SM = os.path.join(TTS_MODEL_DIR_SM, AUX_DIR, \"cmudict-0.7b_nv22.08\")\n",
    "\n",
    "# Relative path to Normalization grammer\n",
    "WFST_TOKENIZER_MODEL_SM = os.path.join(TTS_MODEL_DIR_SM, NG_DIR, \"tokenize_and_classify.far\")\n",
    "WFST_VERBALIZER_MODEL_SM = os.path.join(TTS_MODEL_DIR_SM, NG_DIR, \"verbalize.far\")\n",
    "\n",
    "# Relative path where the generated .rmir file will be stored\n",
    "TTS_RMIR_SM = os.path.join(TTS_MODEL_DIR_SM, \"tts.rmir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Riva servicemaker docker to run riva-build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --rm --gpus 0 -v $TTS_MODEL_DIR:$TTS_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "             riva-build speech_synthesis $TTS_RMIR_SM:$KEY \\\n",
    "             $SG_SM:$KEY \\\n",
    "             $VC_SM:$KEY \\\n",
    "             --voice_name ljspeech \\\n",
    "             --abbreviations_file=$ABBR_SM \\\n",
    "             --arpabet_file=$PR_SM \\\n",
    "             --wfst_tokenizer_model=$WFST_TOKENIZER_MODEL_SM \\\n",
    "             --wfst_verbalizer_model=$WFST_VERBALIZER_MODEL_SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments we used above are just an example, and there are many more optional parameter you can configure! For now, let's take a look into what those arguments we used above mean -\n",
    "\n",
    "* General pipeline parameters:\n",
    "    * `--voice_name`: is the name of the model. Defaults to English-US.Female-1.\n",
    "    * `--abbreviations_file`: is the file containing abbreviations and their corresponding expansions\n",
    "    * `--arpabet_file`: is the file containing the pronunciation dictionary mapping from words to their phonetic representation in ARPABET\n",
    "* ITN model specific parameters\n",
    "    * `--wfst_tokenizer_model`: Sparrowhawk model to use for tokenization and classification, must be in .far (finite-state archive) format. \n",
    "    * `--wfst_verbalizer_model`: Sparrowhawk model to use for verbalizer, must be in .far (finite-state archive) format.\n",
    "\n",
    "This information is also accessible through the `riva-build speech_synthesis -h` command, and more information about additional parameters to `riva-build` can be found in the [riva-build optional parameters](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-custom.html#riva-build-optional-parameters) documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --rm $RIVA_SM_CONTAINER -- riva-build speech_synthesis -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the .rmir\n",
    "!ls -lt $TTS_MODEL_DIR/*.rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model repostory relative to the SM docker\n",
    "MODEL_REPO_SM = os.path.join(TTS_MODEL_DIR_SM, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $TTS_MODEL_DIR:$TTS_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  $TTS_RMIR_SM:$KEY $MODEL_REPO_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the models directory\n",
    "!ls -lt $TTS_MODEL_DIR/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Skills Quick Start resources from NGC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Riva Skills Quick Start guide\n",
    "The [Riva Skills Quick Start](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart) guide contains easy-to-use scripts to download and deploy models. \n",
    "\n",
    "`NOTE:` The scripts in Quick Start can download and deploy the default models. We downloaded the TTS models above just to demonstrate how to use Riva ServiceMaker tools, which will be used during customization tutorials to re-deploy the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_QSG = os.path.join(os.getcwd(), \"riva_quickstart_v2.4.0\")\n",
    "\n",
    "# Downloads the quick start directory to a folder in the current directory and uncompresses it\n",
    "if os.path.exists(RIVA_QSG):\n",
    "    print(\"Riva Quick Start guide exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Riva Quick Start guide Model\")\n",
    "    !ngc registry resource download-version \"nvidia/riva/riva_quickstart:2.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Riva Quick Start \n",
    "This configures the scripts to deploy the TTS models we obtained as a result of Riva servicemaker tools in the previous section. <br>\n",
    "For this, we modify the `config.sh` file to enable relevant Riva services (TTS for the FastPitch/HiFiGAN model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $RIVA_QSG/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if above the model repository is generated at `$TTS_MODEL_DIR/models`, then you can specify `riva_model_loc` as the same directory as `TTS_MODEL_DIR`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=false\n",
    "## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=true                                              \n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with the path TTS_MODEL_DIR)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION:**</font> **Make sure to do the following before moving forward:**\n",
    "1. In the file navigator in Jupyter Lab, navigate to riva_quickstart_v2.* and open config.sh\n",
    "2. Configure settings as shown in the snippet above\n",
    "   - Set asr and nlp services to false\n",
    "   - Configure the riva_model_loc path to where the models resulting from riva-deploy are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `riva-model-loc` to where the models resulting from riva-deploy are stored. In our case it is TTS_MODEL_DIR\n",
    "!echo $TTS_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_QSG && chmod +x ./riva_start.sh && chmod +x ./riva_stop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Riva Start to start the server. This will deploy your model(s).\n",
    "! cd $RIVA_QSG && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with the models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, you can install the Riva Python API bindings for the client. This is available as a `pip` [package](https://pypi.org/project/nvidia-riva-client/). Feel free to read more about the python client [here](https://github.com/nvidia-riva/python-clients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Speech Synthesis\n",
    "The following cells queries the Riva server (using gRPC) with an input audio to yield a transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import riva.client # RIVA 2.3.0 and above\n",
    "except:\n",
    "    import riva_api.riva_audio_pb2 as ra # RIVA 2.0.0 and above\n",
    "    import riva_api.audio_pb2 as ra\n",
    "    import riva_api.riva_tts_pb2 as rtts\n",
    "    import riva_api.riva_tts_pb2_grpc as rtts_srv\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following URI assumes a local deployment of the Riva Speech API server is on the default port. In case the server deployment is on a different host or via a Helm chart on Kubernetes, use an appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_tts = riva.client.SpeechSynthesisService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_hz = 44100\n",
    "resp = riva_tts.synthesize(\n",
    "    text = \"Is it recognize speech or wreck a nice beach?\",\n",
    "    language_code = \"en-US\",\n",
    "    encoding = riva.client.AudioEncoding.LINEAR_PCM,    # Currently only LINEAR_PCM is supported\n",
    "    sample_rate_hz = sample_rate_hz,                    # Generate 44.1KHz audio\n",
    "    voice_name = \"ljspeech\"         # The name of the voice to generate\n",
    ")\n",
    "\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you should hear a synthesized audio for the input text. Now you have a speech synthesis pipeline running! \n",
    "\n",
    "In the next notebook, you will look into how you can customize the phoneme and prosody of the same synthesized voice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
